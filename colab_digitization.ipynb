{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpUypy8i646y"
   },
   "source": [
    "# Basics of Digitization\n",
    "*Mikołaj Leszczuk, Jakub Nawała, Grzegorz Pasterczyk*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SzvoqvqwbVy"
   },
   "source": [
    "## Introduction\n",
    "One of the basic tasks in processing and transmitting video signals is signal conversion. In particular, signals are converted from the analogue to digital form. Sometimes, at the same processing stage, visual material acquired is compressed “on-the-fly.” Then, it is stored using one of the popular container formats like MP4 or AVI. This fast, “on-the-fly” compression may not be sufficient for some applications. Thus, further recompression is sometimes necessary (e.g., to fit into predefined streaming bandwidth or satisfy maximum file size requirements).\n",
    "\n",
    "Viewing (i.e., playing) a compressed video requires decompression (most often in real time). Afterwards, moving pictures are transmitted onto a screen (in a digital or, after a conversion, in an analogue form). The processes of digitization and viewing will be performed in the course of this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFVfHxq27CvQ"
   },
   "source": [
    "## Purpose\n",
    "The purpose of this exercise is to give you a basic understanding regarding digitization of analogue video signals. An exemplary analogue video signal is a signal recorded with a USB webcam. (Although the analogue version of the video signal is most often not available to the user of the webcam. This is because the signal gets digitized in the webcam itself and sent to the computer in this digital version only.)\n",
    "\n",
    "Below is a list of topics this notebook touches upon:\n",
    "* frame rate of a video,\n",
    "* colour spaces,\n",
    "* picture histogram(s),\n",
    "* edge detection and\n",
    "* barcode recognition.\n",
    "\n",
    "Note that the last two topics actually show examples of digital image processing (and not the digitization process as such). They are here to show you how the digital version of an image can be used to extract various information (that would be difficult to extract from the analogue version of the image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kINU4Dy7KGw"
   },
   "source": [
    "## Requirements\n",
    "To complete this exercise you will need a computer with an internet connection and, optionally, a camera (if you would like to test algorithms presented here with images of your own).\n",
    "\n",
    "**Importantly, this notebook is known to be working in [the Google Colaboratory platform](https://colab.research.google.com/). It will certainly not work if you try to run it locally under Windows. It is also not advisable to try to run it locally under the Linux operating system (although this may be successful).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0FkMldh7XLG"
   },
   "source": [
    "## Setting Up the Computer\n",
    "The first time you use this notebook you may be asked to allow it to access your camera.\n",
    "Please allow the notebook to access the camera to make sure everything works as expected.\n",
    "\n",
    "The cell below configures appropriately the computer on which this notebook is run. (If you run this through Google Colaboratory then the computer mentioned is some server at Google premises.)\n",
    "\n",
    "**Please run the cell below before running any other cell in this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IyftoBUlS0E"
   },
   "outputs": [],
   "source": [
    "#@title <== Please click this and wait several seconds as we prepare your software environment { display-mode: \"form\" }\n",
    "print('Loading settings... Please wait...')\n",
    "!apt-get -qq install libzbar0\n",
    "!pip3 -q install pyzbar\n",
    "import matplotlib.pyplot as pyplot\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider\n",
    "import pyzbar.pyzbar as pyzbar\n",
    "import urllib.request, urllib.error\n",
    "import os.path\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "\n",
    "def matrix(R, G, B, set_channel):\n",
    "  RGBm = np.zeros([3, 5, 3], dtype=np.uint8)\n",
    "  RGBm[:,:] = [R, G, B]\n",
    "  img = Image.fromarray(RGBm)\n",
    "  file = 'RGBm.jpg'\n",
    "  img.save(file)\n",
    "  cs('RGBm.jpg', set_channel)\n",
    "  \n",
    "def readImage(path):\n",
    "    if os.path.isfile(path):\n",
    "        cap = cv2.imread(path)\n",
    "        oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "        return oryg\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(path)\n",
    "            code = 200\n",
    "        except urllib.error.URLError as e:\n",
    "            code = 'Name resolve problem'\n",
    "        except urllib.error.HTTPError as e:\n",
    "            code = e.code\n",
    "        if code == 200:\n",
    "            req = urllib.request.urlopen(urllib.request.Request(path, headers={'User-Agent': 'Mozilla/82.0'}))\n",
    "            arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "            cap = cv2.imdecode(arr, -1)\n",
    "            oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "            return oryg\n",
    "        else:\n",
    "            print(code)\n",
    "            #cap = cv2.imread('digitization_1/files/http_error.png')\n",
    "            #oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "            #return oryg\n",
    "        print(code)\n",
    "\n",
    "def ploting2(toPlot1, title1, toPlot2, title2, cmap):\n",
    "    pyplot.figure(figsize=[15.0, 5.0])\n",
    "    pyplot.subplot(1,2,1)\n",
    "    pyplot.title(title1)\n",
    "    pyplot.imshow(toPlot1)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.subplot(1,2,2)\n",
    "    pyplot.title(title2)\n",
    "    pyplot.imshow(toPlot2, cmap)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.show()\n",
    "\n",
    "def cs(image, set_channel):\n",
    "    oryg = readImage(image)\n",
    "    title1 = 'Original image'\n",
    "    title2 = 'Channel {} of original image'.format(set_channel)\n",
    "    cmap = 'viridis'\n",
    "\n",
    "    if set_channel in {'R','G','B'}:\n",
    "        h, w = oryg.shape[0], oryg.shape[1]\n",
    "        zeros = np.zeros((h,w), dtype=\"uint8\")\n",
    "        r, g, b = cv2.split(oryg)\n",
    "        if set_channel == 'R':\n",
    "            typ = cv2.merge((r,zeros,zeros))\n",
    "        elif set_channel == 'G':\n",
    "            typ = cv2.merge((zeros,g,zeros))\n",
    "        elif set_channel == 'B':\n",
    "            typ = cv2.merge((zeros,zeros,b))\n",
    "\n",
    "\n",
    "    elif set_channel in {'Y','Cr','Cb'}:\n",
    "        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YCrCb)\n",
    "        cmap = 'gray'\n",
    "        if set_channel == 'Y':\n",
    "            typ = cv2.split(frame)[0]\n",
    "        elif set_channel == 'Cb':\n",
    "            typ = cv2.split(frame)[2]\n",
    "        elif set_channel == 'Cr':\n",
    "            typ = cv2.split(frame)[1]\n",
    "\n",
    "\n",
    "    elif set_channel in {'Y_','U','V'}:\n",
    "        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YUV)\n",
    "        cmap = 'gray'\n",
    "        if set_channel == 'Y_':\n",
    "            typ = cv2.split(frame)[0]\n",
    "        elif set_channel == 'U':\n",
    "            typ = cv2.split(frame)[1]\n",
    "        elif set_channel == 'V':\n",
    "            typ = cv2.split(frame)[2]\n",
    "\n",
    "\n",
    "    elif set_channel in {'H','S','V_'}:\n",
    "        frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2HSV)\n",
    "        cmap = 'gray'\n",
    "        if set_channel == 'H':\n",
    "            typ = cv2.split(frame)[0]\n",
    "        elif set_channel == 'S':\n",
    "            typ = cv2.split(frame)[1]\n",
    "        elif set_channel == 'V_':\n",
    "            typ = cv2.split(frame)[2]\n",
    "\n",
    "\n",
    "    elif set_channel in {'RGB','YCbCr','YUV','HSV'}:\n",
    "        title2 = 'Original image in {} color space'.format(set_channel)\n",
    "        if set_channel == 'RGB':\n",
    "            typ = oryg\n",
    "        elif set_channel == 'YCbCr':\n",
    "            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YCrCb)\n",
    "            cap = cv2.cvtColor(frame, cv2.COLOR_YCrCb2RGB)\n",
    "            typ = cap\n",
    "        elif set_channel == 'YUV':\n",
    "            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2YUV)\n",
    "            cap = cv2.cvtColor(frame, cv2.COLOR_YUV2RGB)\n",
    "            typ = cap\n",
    "        elif set_channel == 'HSV':\n",
    "            frame = cv2.cvtColor(oryg, cv2.COLOR_RGB2HSV)\n",
    "            cap = cv2.cvtColor(frame, cv2.COLOR_HSV2RGB)\n",
    "            typ = cap\n",
    "    ploting2(oryg, title1, typ, title2, cmap)\n",
    "\n",
    "def histogram(image):\n",
    "    oryg = readImage(image)\n",
    "    h, w = oryg.shape[0], oryg.shape[1]\n",
    "    print('Loaded image size is {}x{}'.format(w, h))\n",
    "    color = ('r','g','b')\n",
    "    pyplot.figure(figsize=[15.0, 5.0])\n",
    "    pyplot.subplot(1,2,1)\n",
    "    pyplot.title('Original image')\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(oryg)\n",
    "    pyplot.subplot(1,2,2)\n",
    "    pyplot.xlabel('Pixels value')\n",
    "    pyplot.ylabel('Number of pixels')\n",
    "    pyplot.title('Histogram of original image')\n",
    "    for i,color in enumerate(color):\n",
    "        hist = cv2.calcHist([oryg],[i],None,[256],[0,256])\n",
    "        pyplot.plot(hist,color = color)\n",
    "        pyplot.xlim([0,256])\n",
    "    pyplot.show()\n",
    "\n",
    "def edges(image):\n",
    "    oryg = readImage(image)\n",
    "    edges = cv2.Canny(oryg,100,200)\n",
    "    title1 = 'Original image'\n",
    "    title2 = 'Canny edge detecting image output'\n",
    "    cmap = 'viridis'\n",
    "    ploting2(oryg,title1,edges,title2,cmap)\n",
    "\n",
    "def scanCodes(image):\n",
    "    oryg = readImage(image)\n",
    "    image = oryg.copy()\n",
    "    decodedObjects = pyzbar.decode(oryg)\n",
    "    if decodedObjects != '':\n",
    "        for obj in decodedObjects:\n",
    "            print('Type of code : ', obj.type)\n",
    "            print('Data on code: ', obj.data,'\\n')\n",
    "    else:\n",
    "      print('Unable to read code. Please try another image.')\n",
    "    for decodedObject in decodedObjects:\n",
    "        points = decodedObject.polygon\n",
    "        if len(points) > 4 :\n",
    "            hull = cv2.convexHull(np.array([point for point in points]))#, dtype=np.float32))\n",
    "            hull = list(map(tuple, np.squeeze(hull)))\n",
    "        else :\n",
    "            hull = points;\n",
    "\n",
    "        n = len(hull)\n",
    "\n",
    "        for j in range(0,n):\n",
    "            cv2.line(image, hull[j], hull[ (j+1) % n], (255,0,0), 3)\n",
    "    title1 = 'Original image'\n",
    "    title2 = 'Image with surrounded barcode'\n",
    "    cmap = 'viridis'\n",
    "    ploting2(oryg,title1,image,title2,cmap)\n",
    "\n",
    "def make_file(select_video,frame_rate):\n",
    "    path = 'digitization_1/files/{}.mp4'.format(select_video)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    size = int(cap.get(3)),int(cap.get(4))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP08')\n",
    "    out = cv2.VideoWriter('frame_rates.webm', fourcc, frame_rate, size)\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret==True:\n",
    "            out.write(frame)\n",
    "        else:\n",
    "            break\n",
    "    files.download('frame_rates.webm')\n",
    "\n",
    "##### Ćwiczenie z HDR\n",
    "\n",
    "def show_oryg(img_1, img_2, img_3):\n",
    "\n",
    "    pyplot.figure(figsize=[12.0, 8.0])\n",
    "    pyplot.subplot(1,3,1)\n",
    "    pyplot.imshow(img_1)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.subplot(1,3,2)\n",
    "    pyplot.imshow(img_2)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.subplot(1,3,3)\n",
    "    pyplot.imshow(img_3)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.suptitle('Original taken images', x=0.5, y=0.65, fontsize=21)\n",
    "\n",
    "def plotingHDR(hdr_out, hdr_type):\n",
    "    pyplot.figure(figsize=[7.5, 5.0])\n",
    "    pyplot.title('Image created by {} HDR type.'.format(hdr_type.upper()))\n",
    "    pyplot.imshow(hdr_out)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.show()\n",
    "\n",
    "def loadFilesHDR(image):\n",
    "    img_fn = ['digitization_1/files/{}{}.png'.format(image,str(num)) for num in range(1,4)]\n",
    "    img_list = [cv2.imread(fn) for fn in img_fn]\n",
    "    img_list_ok = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in img_list]\n",
    "    return img_list_ok;\n",
    "\n",
    "def hdr(image, hdr_type):\n",
    "    img_list_ok = loadFilesHDR(image)\n",
    "    img_1, img_2, img_3 = img_list_ok\n",
    "    show_oryg(img_1, img_2, img_3)\n",
    "    exposure_times = np.array([50.0, 11.0, 3.0],dtype=np.float32) #, dtype=np.float32\n",
    "    tonemap1 = cv2.createTonemap(gamma=2.5)\n",
    "\n",
    "    if hdr_type == 'robertson':\n",
    "        merge_robertson = cv2.createMergeRobertson()\n",
    "        hdr_robertson = merge_robertson.process(img_list_ok, times=exposure_times.copy())\n",
    "        res_robertson = tonemap1.process(hdr_robertson.copy())\n",
    "        hdr_out = np.clip(res_robertson*255, 0, 255).astype('uint8')\n",
    "\n",
    "\n",
    "    elif hdr_type == 'mertens':\n",
    "        merge_mertens = cv2.createMergeMertens()\n",
    "        res_mertens = merge_mertens.process(img_list_ok)\n",
    "        hdr_out = np.clip(res_mertens*255, 0, 255).astype('uint8')\n",
    "\n",
    "    elif hdr_type == 'debevec':\n",
    "        time = exposure_times\n",
    "        merge_debevec = cv2.createMergeDebevec()\n",
    "        hdr_debevec = merge_debevec.process(img_list_ok, times=time.copy())\n",
    "        res_debevec = tonemap1.process(hdr_debevec.copy())\n",
    "        hdr_out = np.clip(res_debevec*255, 0, 255).astype('uint8')\n",
    "\n",
    "    else:\n",
    "        print('Błąd funkcji \"hdr\".')\n",
    "\n",
    "    plotingHDR(hdr_out, hdr_type)\n",
    "\n",
    "# Lista plików\n",
    "file_list = [('3color.jpg','https://drive.google.com/uc?export=view&id=1vKZaRSwv5DpTx_76FlqCQKE_eDF6Q3T2'),\n",
    "             ('barcode1.jpg','https://drive.google.com/uc?export=view&id=1sCYCPs-QBae6AjAltHVB08ykmMwPGPgP'),\n",
    "             ('barcode2.jpg','https://drive.google.com/uc?export=view&id=1EWbYCnzMcLwR4yuQouyKEFw5PoTmhYUr'),\n",
    "             ('barcode3.jpg','https://drive.google.com/uc?export=view&id=1JoZV9ZszpA0Inoydl5o98NdsuHmLR9kh'),\n",
    "             ('blue.jpg','https://drive.google.com/uc?export=view&id=15XvO52rMHkxT51MTZda5dRMyiNCcGK_w'),\n",
    "             ('cariage.mp4','https://drive.google.com/uc?export=view&id=1w28ECOGApsGyD8z8-hpX8KiPK-9KeFZF'),\n",
    "             ('garden1.png','https://drive.google.com/uc?export=view&id=1lrZObN37yLliy5I8HIahO7zAC9gcTYUx'),\n",
    "             ('garden2.png','https://drive.google.com/uc?export=view&id=1Q25X5Y6kR-wYFH3NPmiaYyHSGmGNwMNW'),\n",
    "             ('garden3.png','https://drive.google.com/uc?export=view&id=1B08O5wM_1Ou4FnM_dXSjvEUJl39iiePx'),\n",
    "             ('glasses.mp4','https://drive.google.com/uc?export=view&id=1rk1GnVFv4jWFo1snIaFVvOOj_UqepkR6'),\n",
    "             ('green.jpg','https://drive.google.com/uc?export=view&id=19CDtv3bCQyinvSPDMwcsus-GQ-1AnX0k'),\n",
    "             ('lemon.mp4','https://drive.google.com/uc?export=view&id=1EnN_8QGy0JuO1VQgUMN02E2eKfz8Ic5U'),\n",
    "             ('mount1.png','https://drive.google.com/uc?export=view&id=1BzIjwJ8XAI2jSMw0Pxm8l9eMl8oMWQPV'),\n",
    "             ('mount2.png','https://drive.google.com/uc?export=view&id=1p37lrubVLYZzCBUEHls41-zsb9GJ4Ry0'),\n",
    "             ('mount3.png','https://drive.google.com/uc?export=view&id=1KFghmsFiHR4WCt_0eaP1amHkpWYVfawm'),\n",
    "             ('overexp.jpg','https://drive.google.com/uc?export=view&id=1wxIoW0552ECXvrz0wlmKasijLdynLh9N'),\n",
    "             ('qrcode1.jpg','https://drive.google.com/uc?export=view&id=1sCW-UgeSEy7K1zFhcBof1kGKHB45hNbR'),\n",
    "             ('qrcode2.jpg','https://drive.google.com/uc?export=view&id=1oQC7_wzxUTuv1GWpN1s7jdkIGhoIQ8dk'),\n",
    "             ('qrcode3.jpg','https://drive.google.com/uc?export=view&id=183vrnHsI4vGmAFdx7l8JDgR-hOVISl-7'),\n",
    "             ('red.jpg','https://drive.google.com/uc?export=view&id=13soMaT5F_T4FLkkJEsoTS24arNgkG-la'),\n",
    "             ('rgb.jpg','https://drive.google.com/uc?export=view&id=1R3c1YOPpY6M8N2xkpCrnArZ42_KkC5e1'),\n",
    "             ('shapes.jpg','https://drive.google.com/uc?export=view&id=1t16Qen9duRJ0Yzm1YNlJZNAsgGQlFL-M'),\n",
    "             ('underexp.jpg','https://drive.google.com/uc?export=view&id=15yc0jqwj7bt4nZ3FBNcykuMQyNd891s3'),\n",
    "             ('yosemite1.png','https://drive.google.com/uc?export=view&id=1p-JeUkpsQ_Z40ztIZrxwsNi9KZyGH-jn'),\n",
    "             ('yosemite2.png','https://drive.google.com/uc?export=view&id=1Dxtsq_IXMQQzeCC4TXYHL8Psxk_-KWC8'),\n",
    "             ('yosemite3.png','https://drive.google.com/uc?export=view&id=1oZbVUZKTrVlcBOuqLJfg0KZO1-z_AuPc')]\n",
    "# Załadowanie plików\n",
    "!mkdir -p digitization_1/files\n",
    "i = 0\n",
    "for line in file_list:\n",
    "  if (i % 6) == 0:\n",
    "    print('Loading files... ({} of 26)'.format(i))\n",
    "  i+=1\n",
    "  filename = line[0]\n",
    "  url = line[1]\n",
    "  !wget -q -O digitization_1/files/$filename \"$url\"\n",
    "print('All files loaded ({} of 26).\\nYou can continue.'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiubEhglkDe_"
   },
   "source": [
    "## Frame Rate\n",
    "Please run the cell below to generate versions of various videos with a frame rate of your choice.\n",
    "\n",
    "From the dropdown menu (which will be visible once you run the cell below) choose one of example videos and set number of frames per second through the slide bar. A video file will be downloaded automatically. Please open it and observe changes. Please note that each change on the slide bar will generate a new file.\n",
    "\n",
    "<font color=blue>Report: </font>Please compare videos with the following frame rates and write\n",
    "down your conclusions in the report: 5, 15, 25, 50, 100 and 150. Can you see any difference between 100\n",
    "and 150 FPS (frames per second) versions of the video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTAiLlOD7Hho"
   },
   "outputs": [],
   "source": [
    "#@title Exercise { display-mode: \"form\" }\n",
    "interact(make_file, select_video = [('Falling fruit','lemon'),('View on street','cariage'),('Reading people','glasses')], frame_rate = IntSlider(min=5, max=150, step=5, continuous_update=False));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyFNrzgIkDfK"
   },
   "source": [
    "## Colour Spaces\n",
    "\n",
    "Please execute the first cell below and experiment with different colour spaces (by choosing RGB, YUV, YCbCr or HSV in the *set_channel* dropdown menu). Please note what happpens when you ask the programme to convert the input to a colour space of choice. Does this influence the way the image looks?\n",
    "\n",
    "Now, experiment with the programme asking it to display the output image using only one colour channel of a given colour space. For example, by choosing 'R' in the *set_channel* dropdown menu you ask the programme to display only the 'R' channel of the RGB colour space. Does this operation influence the way the output image looks? Do you understand why the output looks as it does? (If not, please contact the lecturer or look for more information regarding the colour spaces we use here on-line.)\n",
    "\n",
    "Please pay special attention to blue objects when displaing just the Cb channel (a.k.a. 'blue difference') of the YCbCr colour space. Note that the blue objects are significantly whiter than other objects. (The same can be observed for red objects and the Cr channel.)\n",
    "\n",
    "<b>How to Interpret Output Images?</b><br>\n",
    "As you know, each pixel in the RGB colour space has three values, one for each channel (Red, Green and Blue). Grayscale images have only one channel and one value for each pixel.\n",
    "\n",
    "Output images (in the code cell below) for a single channel (e.g., the Y channel\n",
    "from the YCbCr colour space) are shown as grayscale images. Pixels with the value\n",
    "of 0 are black. Pixels with higher values are brighter, and those close to the\n",
    "maximum value (255) are white. The only exception to this rule are output\n",
    "images for R, G and B channels of the RGB colour space. Those are shown as\n",
    "true-colour images with all irrelevant channels set to 0. For example, when\n",
    "displaying the ouput image for the R channel, G and B channels in the output\n",
    "image are set to 0.\n",
    "\n",
    "<b>RGB to YCbCr Conversion</b><br>\n",
    "The equations below help interpret images using the YCbCr colour space. (Due to\n",
    "historical reasons Cb and Cr channels are sometimes referred to as Pb and Pr\n",
    "channels, respectively.)\n",
    "\n",
    "$Y'=K_R*R'+K_G*G'+K_B*B'$,\n",
    "\n",
    "$P_B=\\frac{1}{2}\\frac{B'-Y'}{1-K_B}$,\n",
    "\n",
    "$P_R=\\frac{1}{2}\\frac{R'-Y'}{1-K_R}$,\n",
    "\n",
    "where $K_R$, $K_G$, and $K_B$ are ordinarily derived from the definition of the corresponding RGB space, and required to satisfy $K_R+K_G+K_B = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ0B5ioUkDfL"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 1 - run cell and experiment with prepared images { display-mode: \"form\" }\n",
    "interact(cs, image=[('Basic RGB colours','digitization_1/files/3color.jpg'), ('Random shapes', 'digitization_1/files/rgb.jpg'), ('Advantage red color', 'digitization_1/files/red.jpg'), ('Advantage green color', 'digitization_1/files/green.jpg'), ('Advantage blue color', 'digitization_1/files/blue.jpg')], set_channel=[('RGB color space','RGB'),('R','R'),('G','G'),('B','B'),('YCbCr color space','YCbCr'),('Y','Y'),('Cb','Cb'),('Cr','Cr'),('YUV color space','YUV'),('Y','Y_'),('U','U'),('V','V'),('HSV','HSV'),('H','H'),('S','S'),('V','V_')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tLPCSKP7bq0"
   },
   "source": [
    "When you are done experimenting with the cell above you can run the cell below to load any image available on-line and apply to it the same set of transformations. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCH9-6jUkDfL"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2 - run the cell and paste the http address of your image { display-mode: \"form\" }\n",
    "interact(cs, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353569.jpg', set_channel=[('RGB color space','RGB'),('R','R'),('G','G'),('B','B'),('YCbCr color space','YCbCr'),('Y','Y'),('Cb','Cb'),('Cr','Cr'),('YUV color space','YUV'),('Y','Y_'),('U','U'),('V','V'),('HSV','HSV'),('H','H'),('S','S'),('V','V_')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color=blue>Report: </font>Please include in the report an exemplary image\n",
    "and its version with only the Y channel available (from either YCbCr or YUV\n",
    "colour spaces). Is content of the image clearly visible when we only retain the\n",
    "Y channel? Does it look like an image from the black-and-white TV era? Please\n",
    "answer those questions in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxZGd6ZwkDfM"
   },
   "source": [
    "## Picture Histogram\n",
    "This demo uses the 2-D Histogram in Image Processing to calculate the histograms of R, G, and B values.\n",
    "\n",
    "Click inside code-block and click Run button in navigate panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvYBxlJ5kDfM"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 1 - experiment with pre-loaded images { display-mode: \"form\" }\n",
    "interact(histogram, image=[('Advantage red color', 'digitization_1/files/red.jpg'),('Advantage green color', 'digitization_1/files/green.jpg'),('Advantage blue color', 'digitization_1/files/blue.jpg'),('Overexposure image','digitization_1/files/overexp.jpg'),('Underexposure image','digitization_1/files/underexp.jpg')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGjvGOPtkDfN"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\n",
    "interact(histogram, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353585.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color=blue>Report: </font>Please select one image and its histogram and\n",
    "describe in your own words what is the content of the histogram (e.g., What is the\n",
    "meaning of the axes? How many curves are in the histgoram? Why? Is the shape\n",
    "of the curves resembling the uniform distribution?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnOL7G7hkDfO"
   },
   "source": [
    "## Edge Detection\n",
    "The Canny method is applied to find the edges of objects in the input images.\n",
    "\n",
    "Click inside the code cells and click the Run button on the left to perform\n",
    "the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsSEEJ7bkDfO"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 1 - run cell and experiment with pre-loaded images { display-mode: \"form\" }\n",
    "interact(edges, image=[('Random shapes', 'digitization_1/files/shapes.jpg'),('Advantage red color', 'digitization_1/files/red.jpg'),('Advantage green color', 'digitization_1/files/green.jpg'),('Advantage blue color', 'digitization_1/files/blue.jpg'),('Barcode','digitization_1/files/barcode2.jpg'),('QR Code','digitization_1/files/qrcode3.jpg')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC_HOOWGkDfP"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\n",
    "interact(edges, image='https://thumbs.dreamstime.com/b/multi-colours-strips-texture-design-165353532.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color=blue>Report: </font>Please think of a potential application of edge\n",
    "detection. Then briefly describe your proposition in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyxMt9VOkDfP"
   },
   "source": [
    "## Barcode Recognition\n",
    "\n",
    "This is an example of a more advanced image processing algorithm. It detects\n",
    "region of interest (RoI) that contains a barcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fePzFGIikDfQ"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 1 - Experiment with pre-loaded images { display-mode: \"form\" }\n",
    "interact(scanCodes, image=[('QR 1','digitization_1/files/qrcode1.jpg'),('QR 2','digitization_1/files/qrcode2.jpg'),('QR 3','digitization_1/files/qrcode3.jpg'),('BAR 1','digitization_1/files/barcode1.jpg'),('BAR 2','digitization_1/files/barcode2.jpg'),('BAR 3','digitization_1/files/barcode3.jpg')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_f_rhdRkDfR"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2 - run cell and paste http address of your image { display-mode: \"form\" }\n",
    "interact(scanCodes, image='https://barcodesgonewild.com/wp-content/uploads/2014/05/Axe-barcode-1.jpeg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color=blue>Report: </font> Please upload an image with a QR code and check\n",
    "whether the code above properly recognises its content. Add to your report both\n",
    "the image you uploaded and the result of running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2ZaNTF6kDfR"
   },
   "source": [
    "# Digitization - Advanced Topics in Digital Photography\n",
    "*Michał Grega, Mikołaj Leszczuk, Jakub Nawała, Grzegorz Pasterczyk*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcVVyiNQNltq"
   },
   "source": [
    "## Purpose \n",
    "Purpose of this laboratory is to present the RAW processing workflow for digital images and the HDR technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzVguLU1NqPr"
   },
   "source": [
    "## Prerequisites\n",
    "* Basics of digital photography\n",
    "* Basics of image formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8J5iCPQN9ms"
   },
   "source": [
    "## RAW Processing\n",
    "*All photographs are © Michał Grega unless stated otherwise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDoMDSLnOCSK"
   },
   "source": [
    "### What is RAW and why to use it?\n",
    "RAW is a file format used for storing the information on the image taken by the digital camera. It is not an image format. It contains raw (unprocessed) data stored by the physical sensor (radiometric data). Apart from that, a RAW file may contain additional metadata on:\n",
    "* Make and model of the camera,\n",
    "* Physical properties of the sensor,\n",
    "* Exposure and camera settings,\n",
    "* Lens settings,\n",
    "* A highly compressed .jpg thumbnail of the image.\n",
    "\n",
    "Please note, that the camera sensor is most commonly not a pixel matrix (see Fig. 1). It is a CMOS or a CCD (charge-coupled device) sensor covered by a filter (see Fig. 2). Underneath this filter, there is an array of photosensitive subpixels, which do not have to be even of rectangular shape. Therefore in order to convert the radiometric data to an image detailed information on the sensor geometry must be available for the software algorithm.\n",
    "\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ptl8YS1g5EqjQ7_Y0oK3WL1naqTKkOEf\" width=\"500\">\n",
    "\n",
    "Fig. 1. Sensor layouts (Wikipedia)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1O_Y89x3GTTPQ9J8Qqte8In9-DQ4cC43y\" width=\"500\">\n",
    "\n",
    "Fig. 2. Bayer colour filter (do you see anything unexpected?) (Wikipedia)</center></div></body></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuWT40wsOHf3"
   },
   "source": [
    "### How it differs from .jpg or .tiff?\n",
    "A RAW image captured by a camera is an uncompressed and unprocessed raw measurement of light. It is commonly referred to as a digital negative, as it serves a similar purpose as a traditional film negative. A .jpg or .tiff image produced by the camera is that digital negative processed (developed) on the fly by the camera built-in software. The software most typically conducts a set of automated operations:\n",
    "1. develop the raw image (knowing the physical properties of the sensor),\n",
    "* enhance the resulting image (by applying contrast and colour correction and sharpening),\n",
    "* apply additional correction algorithms (e.g. red-eye reduction),\n",
    "* compress the image to the desired format (lossy .jpg or lossless .tiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbFbOWFmOOsd"
   },
   "source": [
    "### What are the benefits of RAW shooting?\n",
    "The most profound and important benefit is that a photographer retains full control of the creation, correction and compression processes. All the adjustments can be made by hand and tuned in order to achieve the desired effect.<br> \n",
    "Moreover, RAW files offer much better input for post-processing, as the state-of-the-art sensors store (digitize) the data at 14 bits per colour per pixel. It means that a raw image can hold 214 shades per colour, meaning 242 total colours. A .jpg file typically saves 8 bits per colour, meaning 28 shades per colour resulting in 224 colours. In short, the RAW format offers better colour fidelity **(18 orders of magnitude greater than .jpg)**, much higher dynamic range (High Dynamic Range imaging will be explained further on) and more data for further corrections.<br>\n",
    "What are the drawbacks of RAW shooting?\n",
    "* The visual quality of an unenhanced RAW file is not satisfying as no corrections are applied,\n",
    "* RAW images are of large size,\n",
    "* RAW images require tedious manual development and correction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daWS7AyNOVXQ"
   },
   "source": [
    "### RAW processing workflow\n",
    "`\n",
    "Disclaimer:\n",
    "Photography is an art and thus slips away from scientific definition. Moreover, it is controversial how much post-processing (a.k.a) “photoshopping” is allowed to a professional. Photographic agencies and photographic competitions have strict rules that define what is allowed and what is not.\n",
    "`\n",
    "The RAW processing workflow consists of several steps - all described below. Each photographer usually creates his/her own workflow by adding or removing some of the steps. It is important to sustain the order of the steps, as there is a logic behind them (e.g. sharpening has to be done prior to development).\n",
    "1. **Cropping and straightening** – a selection of a composition of the image\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18wRyA5FDFnTovss-rfxsa_0ae3M0YICC\" width=\"500\"/>\n",
    "<p>Before and after straightening </p></center></div></body></html>\n",
    "\n",
    "2. **Exposure correction** – done in order to correct for over- or underexposed images. Due to the physical characteristics of the sensor a rule of the thumb is that it is better to shoot under-. rather than, overexposed images as it is easier to compensate for underexposure. The most useful tool is the luminosity histogram. A well-exposed photo covers the whole dynamic range and fills the whole histogram. An under- or overexposed photo shows clipping in (respectively) low or high values.\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1mtBmPzGXFJ2bZQIxpNljrwYL15BOAePe\" width=\"500\"/>\n",
    "<p>A histogram of an underexposed image </p>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=189mjDgxRqx2B96T7HwAYOishQ5rZAwme\" width=\"500\"/>\n",
    "<p>A histogram of an overexposed image</p>\n",
    "<p>(gray area - total luminosity; red, green and blue curves - luminosity for each RGB channel)</p>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1i8B-2KZ4hwshZPAJXDEuDU5zA0FMCX-Z\" width=\"500\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ZqMOJaDyymCdckE8OsBKXco0pK-uf6Aj\" width=\"500\"/>\n",
    "<p>Before and after exposure correction. Notice the histogram.</p>\n",
    "</center></div></body></html>\n",
    "\n",
    "More advanced software allows for a software-based increase in dynamic range (i.e. the increase in an end-to-end distance between extreme pixel values). Software algorithm detects the under- or overexposed parts of the image and enhances them instead of modifying the whole image.\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dChZqeL3rcYU_KuTXov6sJqQ6_z-esvv\" width=\"600\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1EDdxReNJgOr6fWlnY2PlMAwDCpiooSxB\" width=\"600\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ngDySajgzyMw0a7jdLrSidXeevu-Vky2\" width=\"600\"/>\n",
    "<p> Exposure correction using overall exposure and software HDR. Notice the contrast between foreground and background.</p>\n",
    "</center></div></body></html>\n",
    "\n",
    "3. **Contrast correction** – increases the contrast in an image. Images captured in the RAW format appear to be flat and not vibrant. That is due to the lack of contrast correction. Contrast is the difference between the brightest and darkest pixel in the image. While it is easy to define, there are many algorithms that aim at improving contrast by maintaining the general luminosity and colours of the image. \n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1P2nc32N7ok7vxQhGKGhJ976SJsJj8dat\" width=\"600\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1GWIqYDuiVI9sE_nPkkCig6qnuSTGX-zX\" width=\"600\"/>\n",
    "<p> Contrast correction </p>\n",
    "</center></div></body></html>\n",
    "\n",
    "4. **Colour correction** – shooting an image in given conditions may cause the colours to be distorted. Especially the type of light (sunlight vs artificial) makes the colours unnatural. For example, shooting in artificial incandescent light causes images to be unnaturally warm (due to the high amount of infrared radiation). On the other hand, shooting in full sunlight on high altitudes causes photographs to be unnaturally cool (because of the high amount of UV radiation). It can be compensated for using white balance compensation.\n",
    "\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1VdqrzX6ojjP5w0xrDm3l109Is9bv9ASu\" width=\"600\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1lVOL2GqRPdqUK3pBkoilNPkXJ69KKSCs\" width=\"600\"/>\n",
    "<p> White balance compensation – notice the clipping on a histogram in the red channel. </p>\n",
    "</center></div></body></html>\n",
    "\n",
    "5. **Sharpening and detail** – allows to sharpen the image and remove unnecessary artefacts. Among those are spots caused by dirt on the sensor (or lenses) and noise generated by the sensor itself.\n",
    "\n",
    "<html><body><div><center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1xKAT3EMW0kE0TfePj4p6Ty_g7izh-kx6\" width=\"600\"/>\n",
    "<p> Sharpening and noise reduction </p>\n",
    "</center></div></body></html>\n",
    "\n",
    "6. **Development** – allows converting the image to the target format and colour space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9ozYL_PO4qo"
   },
   "source": [
    "### RAW processing exercise\n",
    "1. You can use any RAW processing software you wish. Note, however, that the paid software usually offers a more intuitive interface and more advanced algorithms. If you own a DSLR (Digital Single-Lens Reflex) camera you probably got a copy of the manufacturer’s software. Other (costly) solutions are Adobe Photoshop with Lightroom or Capture One (for the use in the laboratory you have to download a version from https://www.phaseone.com/en/Download.aspx).<br>You can also use (free) http://rawtherapee.com/.<br>Examples shown in the previous section were prepared using Capture One Pro, which offers a free 30-day trial.\n",
    "But also you can see exercise below to see how RAW processing works on 3 examples.\n",
    "2. Download example RAW files (see the “RAW Examples” folder accompanying this instruction).\n",
    "3. Develop these RAW files into *.jpg images for web publishing trying to achieve the best visual result. Correct the composition, exposure and colours of the image. Apply sharpening and the correct developmental recipe. Observe what happens when you use high values of the corrections for sharpness, software HDR, exposition. There is a saying for beginners “Set up your sliders in a position that makes your photograph look good and then reduce all by half”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veqepdyeO76B"
   },
   "source": [
    "## HDR Imaging\n",
    "As you might have noticed, one of the most challenging scenes is those with high contrast between shadowy and bright regions. Each optical device, including the human eye, has a dynamic range. A dynamic range is a difference measured in EV units between the darkest and brightest part of the image that shows detail. Increase of one EV unit represents a situation where the amount of light is doubled. A human eye and a modern DSLR camera sensor have a dynamic range of approx. 14 EV (called “stops”). It means that we can double the amount of light 14 times between the brightest and darkest part of the image and still see detail.<br>We can control which part of the scene is covered by our EV range by adjusting the shutter speed, aperture or ISO value of the sensor. We, however, cannot increase this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51iDuQX8O_FP"
   },
   "source": [
    "### What is HDR?\n",
    "HDR, High Dynamic Range, is a photographing technique, in which a set of images is made with different camera setups. Each photograph covers a limited dynamic range, but a combination of the photographs covers a higher dynamic range resulting in an HDR photograph.<br>Of course, the display or printout also has a limited dynamic range, thus a mapping from the wider to the narrower dynamic range has to be done. This is referred to as “tone mapping”.<br>For an exemplary usage of the HDR technique, please take a look at the example below.\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?export=view&id=1a6b01LPYeYMQnNGcYSUY62Zhp69RV0QI' width=\"600\"/>\n",
    "<p>HDR input images. Note, that it was impossible to get details both on the bright (sky) and dark (shadows) areas in any single photo.</p><br><br>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1Q3sgo3W0GjGu4i9_4os3qFj5-we1HKw4' width=\"600\"/>\n",
    "<p>HDR result. Notice, that it looks unnatural, as it shows a higher dynamic range than a human eye is able to process.</p></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLDvMx_MkDfR"
   },
   "source": [
    "### High Dynamic Range (HDR) Exercise\n",
    "Here you can see how HDR works in practice. This exercise uses three different HDR methods:\n",
    "1. Debevec,\n",
    "1. Robertson and\n",
    "1. Mertens.\n",
    "\n",
    "You can test each of them with image triplets coming from the three shots presented in the table below.\n",
    "Each triplet contains three images of the same shot, each with a different exposure time.\n",
    "\n",
    "The table below presents one image from each triplet. Specifically, the imate taken with average\n",
    "exposure time.\n",
    "\n",
    "Run the code cell below the table to test each HDR method with each shot.\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\">Yosemite</th>\n",
    "    <th style=\"text-align:center\">Garden</th>\n",
    "    <th style=\"text-align:center\">Mount</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"https://drive.google.com/uc?export=view&id=1NothOGCG0HYhn6iyAo2w2bbPnO63zvma\"/></td>\n",
    "    <td><img src=\"https://drive.google.com/uc?export=view&id=15SvrRxCbOHYHeLQ8t-b0jw6RK2IRQVSX\"/></td>\n",
    "    <td><img src=\"https://drive.google.com/uc?export=view&id=1ITqflKJ3T0I2BPcRQAIo-H-d_M46cjKh\"/></td></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "170rQRYbkDfY"
   },
   "outputs": [],
   "source": [
    "#@title Exercise  - run cell and experiment with pre-loaded images { display-mode: \"form\" }\n",
    "interact(hdr, image=['yosemite','garden','mount'], hdr_type=['robertson','mertens','debevec']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7DFG-vs7rls"
   },
   "source": [
    "# Supplemental Exercise\n",
    "Please take several photos using your camera. Please try to take photos that:\n",
    "1. are overexposed,\n",
    "2. are underexposed,\n",
    "3. has either red, green or  blue as a dominant colour.\n",
    "\n",
    "Now, use the code cell below to generate histograms for each image.\n",
    "\n",
    "<font color=blue>Report: </font> Please attach the generated histograms to your report.\n",
    "Please also describe verbally what differences do you see between the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJvlzgQK7tRQ"
   },
   "outputs": [],
   "source": [
    "#@title <--- HOMEWORK - please run this cell { display-mode: \"form\" }\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import cv2\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "def histogram(image):\n",
    "    cap = cv2.imread(image)\n",
    "    oryg = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "    h, w = oryg.shape[0], oryg.shape[1]\n",
    "    print('Loaded image size is {}x{}'.format(w, h))\n",
    "    color = ('r','g','b')\n",
    "    pyplot.figure(figsize=[15.0, 5.0])\n",
    "    pyplot.subplot(1,2,1)\n",
    "    pyplot.title('Original image')\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(oryg)\n",
    "    pyplot.subplot(1,2,2)\n",
    "    pyplot.xlabel('Pixels value')\n",
    "    pyplot.ylabel('Number of pixels')\n",
    "    pyplot.title('Histogram of original image')\n",
    "    for i,color in enumerate(color):\n",
    "        hist = cv2.calcHist([oryg],[i],None,[256],[0,256])\n",
    "        pyplot.plot(hist,color = color)\n",
    "        pyplot.xlim([0,256])\n",
    "    pyplot.show()\n",
    "def take_photo(filename='photo.jpg', quality=0.95):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename\n",
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo()\n",
    "  # Show the image which was just taken.\n",
    "  #display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))\n",
    "histogram(filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kopia notatnika Kopia notatnika colab_digitization.ipynb",
   "provenance": [
    {
     "file_id": "13x0VHVgnEgVfkd6geKBhQHQQ7AZcwT_w",
     "timestamp": 1615326265543
    },
    {
     "file_id": "1kbMMujTQJJKHsZoVI8I9WoNao7h-L_TP",
     "timestamp": 1615326037697
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
